1:Reddit Averages Performance Analysis â€“ Timing Summary

Environment: Windows PowerShell, Spark (--master="local[1])
Datasets: reddit-0 (baseline), reddit-1 (performance tests)

Timing Results
reddit-0 (baseline): 12.04s

Reflects Spark startup/shutdown overhead.
reddit-1 (no schema, no caching): 9.84s

Spark reads data twice (schema inference + processing).
Averages recalculated for both outputs.
reddit-1 (schema, no caching): 8.69s

Single read due to schema.
Averages still recalculated.
reddit-1 (schema + caching): 13.01s

2: Based on the timing results above, the most efficient version for this dataset size 
was actually the schema-only version (8.69 seconds), followed by the no-optimizations 
version (9.84 seconds).

The fully optimized version (13.01 seconds) performed slower than expected, likely due to:
- Caching overhead being more significant than benefits for this small dataset
- The reddit-1 dataset may not be large enough to demonstrate caching benefits
- Single-threaded execution (local[1]) limiting parallelization benefits

Time Analysis - File Reading vs. Calculation:
=============================================

Comparing the different versions:
- Schema improvement: 9.84s - 8.69s = 1.15 seconds saved
- The baseline (reddit-0): 12.04 seconds is mostly Spark startup overhead

The relatively small differences between optimized and unoptimized versions 
(1-4 seconds) compared to the baseline startup time (12 seconds) suggests that:

**Most of the time is spent on Spark infrastructure overhead rather than actual 
data processing or file reading.** 

For the reddit-1 dataset size, the file reading and average calculations are 
relatively fast operations compared to JVM startup, Spark context initialization, 
and job coordination overhead.

This demonstrates that Spark's benefits become more apparent with larger datasets 
where the processing time dominates the startup overhead.

Conclusion:
===========

For small datasets like reddit-1, Spark startup overhead dominates execution time.
The schema optimization provides measurable benefits by eliminating duplicate file reads.
Caching benefits would likely become more apparent with larger datasets or more complex 
operations that reuse DataFrames multiple times.

3:cache().  should add right after creating the english_pages DataFrame. This is the optimal place because:

english_pages is used first in the groupBy('hour').agg() operation to create max_views_per_hour
then it's used again in the join operation (as pages_alias)